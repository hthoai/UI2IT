# Model settings
model_checkpoint_interval: 5 # epochs
log_image_interval: 1000 # iters
num_image_log: 1
task: ["clear2snowy", "snowy2clear"]
seed: 21
batch_size: 1 # size of a training mini-batch
epochs: 200 # number of training epochs
decay_epoch: 100 # epoch to start linearly decaying the learning rate to 0
model:
  name: CycleGAN
  parameters:
    nc: 3 # number of channels
    nz: 3 # size of z latent vector
    ngf: 64 # size of feature maps in generator
    ndf: 64 # size of feature maps in discriminator
    ng_blocks: 9 # the number of Residual blocks
    nd_layers: 3 # the number of conv layers in the discriminator
    ksize_d: 4 # kernel size of conv layer in the discriminator
    norm_type: instance # normalization layer `batch` | `instance`
    lambda_A: 10. # forward cycle loss weight
    lambda_B: 10. # backward cycle loss weight
    lambda_idt: 0.5 # identity loss weight
criterion:
  gan:
    name: MSELoss
    parameters:
      reduction: mean
  cycle:
    name: L1Loss
    parameters:
      reduction: mean
  idt:
    name: L1Loss
    parameters:
      reduction: mean
optimizer:
  G:
    name: Adam
    parameters:
      lr: 0.0002 # initial learning rate
      betas: !!python/tuple [0.5, 0.999]
  D:
    name: Adam
    parameters:
      lr: 0.0002 # initial learning rate
      betas: !!python/tuple [0.5, 0.999]

# Dataset settings
datasets:
  train:
    name: UnalignedDataset
    parameters:
      root: "datasets/clear2snowy3/train"
      img_size: [256, 256]
      transformations:
        - name: RandomCrop
          parameters:
            size: 256
        - name: RandomHorizontalFlip
          parameters:
            p: 0.5
        - name: Normalize
          parameters:
            mean: !!python/tuple [0.5, 0.5, 0.5]
            std: !!python/tuple [0.5, 0.5, 0.5]
